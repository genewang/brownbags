{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b324d41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "California Housing Price Prediction Demo\n",
    "======================================\n",
    "\n",
    "This script demonstrates a complete machine learning pipeline for predicting\n",
    "housing prices in California using the California Housing dataset.\n",
    "The script can be converted to a Jupyter notebook using:\n",
    "    jupyter nbconvert --to notebook --execute housing_price_prediction.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f05163d",
   "metadata": {},
   "source": [
    "1. Setup and Dependencies\n",
    "------------------------\n",
    "First, let's import all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce6527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import joblib\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ea718",
   "metadata": {},
   "source": [
    "2. Load and Explore the Data\n",
    "---------------------------\n",
    "We'll use the California Housing dataset, which contains information about\n",
    "housing districts in California from the 1990 Census."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9edc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "california = fetch_california_housing()\n",
    "df = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "df['MedHouseVal'] = california.target * 100000  # Convert to actual dollar values\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nDataset Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head().to_string())\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(df.describe().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f1b3b8",
   "metadata": {},
   "source": [
    "3. Data Visualization\n",
    "--------------------\n",
    "Let's visualize the data to understand the distributions and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f02ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure and axes\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Define units for each feature\n",
    "units = {\n",
    "    'MedInc': ' ($10,000s)',\n",
    "    'HouseAge': ' (years)',\n",
    "    'AveRooms': ' (rooms)',\n",
    "    'AveBedrms': ' (bedrooms)',\n",
    "    'Population': ' (people)',\n",
    "    'AveOccup': ' (people/room)',\n",
    "    'Latitude': ' (degrees)',\n",
    "    'Longitude': ' (degrees)',\n",
    "    'MedHouseVal': ' ($)'\n",
    "}\n",
    "\n",
    "# Plot histograms for all features\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    \n",
    "    # Apply log scale for specific columns with wide ranges\n",
    "    if col in ['AveRooms', 'AveBedrms', 'Population', 'AveOccup']:\n",
    "        plt.xscale('log')\n",
    "        \n",
    "    sns.histplot(df[col], kde=True, bins=30)\n",
    "    \n",
    "    # Add units to x-axis label\n",
    "    unit = units.get(col, '')\n",
    "    plt.xlabel(f'{col}{unit}')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    \n",
    "    # Rotate x-tick labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.suptitle('Feature Distributions', y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.subplots_adjust(top=0.92)\n",
    "plt.show()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df.corr()\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "# Create a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(correlation_matrix, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .8}, annot=True, \n",
    "            fmt=\".2f\", annot_kws={\"size\": 10})\n",
    "\n",
    "# Improve readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.title('Correlation Matrix of Housing Features', pad=20, fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbddf339",
   "metadata": {},
   "source": [
    "4. Data Preprocessing\n",
    "--------------------\n",
    "Prepare the data for modeling by handling missing values, encoding\n",
    "categorical variables, and scaling features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6622b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop('MedHouseVal', axis=1)\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612c39a",
   "metadata": {},
   "source": [
    "5. Model Training\n",
    "----------------\n",
    "We'll train and evaluate multiple regression models to predict housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d503503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    'XGBoost': XGBRegressor(random_state=42, n_jobs=-1, eval_metric='rmse')\n",
    "}\n",
    "\n",
    "# Dictionary to store model performance\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train_scaled, y_train, \n",
    "        cv=5, scoring='neg_mean_squared_error', n_jobs=-1\n",
    "    )\n",
    "    cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'cv_rmse': cv_rmse\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{name} Performance:\")\n",
    "    print(f\"  RMSE: ${rmse:,.2f}\")\n",
    "    print(f\"  MAE: ${mae:,.2f}\")\n",
    "    print(f\"  R²: {r2:.4f}\")\n",
    "    print(f\"  CV RMSE: ${cv_rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4568be",
   "metadata": {},
   "source": [
    "6. Model Comparison\n",
    "------------------\n",
    "Let's compare the performance of all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79478db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'RMSE': [results[m]['rmse'] for m in results],\n",
    "    'MAE': [results[m]['mae'] for m in results],\n",
    "    'R²': [results[m]['r2'] for m in results],\n",
    "    'CV RMSE': [results[m]['cv_rmse'] for m in results]\n",
    "}).sort_values('RMSE')\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Plot model performance\n",
    "plt.figure(figsize=(14, 6))\n",
    "metrics = ['RMSE', 'MAE', 'R²']\n",
    "\n",
    "for i, metric in enumerate(metrics, 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    ax = sns.barplot(x='Model', y=metric, data=results_df, palette='viridis')\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for p in ax.patches:\n",
    "        if metric == 'R²':\n",
    "            ax.annotate(f\"{p.get_height():.3f}\", \n",
    "                       (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                       ha='center', va='center', \n",
    "                       xytext=(0, 9), \n",
    "                       textcoords='offset points')\n",
    "        else:\n",
    "            ax.annotate(f\"${p.get_height():,.0f}\", \n",
    "                       (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                       ha='center', va='center', \n",
    "                       xytext=(0, 9), \n",
    "                       textcoords='offset points')\n",
    "    \n",
    "    # Format y-axis for R² differently\n",
    "    if metric == 'R²':\n",
    "        plt.ylim(0, 1.1)\n",
    "        plt.ylabel('R² Score')\n",
    "    else:\n",
    "        # Add dollar sign and format y-axis for monetary values\n",
    "        ax.yaxis.set_major_formatter('${x:,.0f}')\n",
    "        plt.ylabel(f'{metric} (in $)')\n",
    "    \n",
    "    plt.title(f'Model {metric} Comparison', fontweight='bold')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Model Performance Comparison', y=1.05, fontsize=14, fontweight='bold')\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6508f4",
   "metadata": {},
   "source": [
    "7. Feature Importance\n",
    "--------------------\n",
    "Let's analyze which features are most important in our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad316f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "# Plot feature importance\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # For tree-based models\n",
    "    feature_importance = pd.Series(\n",
    "        best_model.feature_importances_,\n",
    "        index=X.columns\n",
    "    ).sort_values(ascending=True)  # Sort ascending for horizontal bar plot\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create a horizontal bar plot\n",
    "    ax = sns.barplot(x=feature_importance.values, y=feature_importance.index, \n",
    "                    palette='viridis_r', orient='h')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(feature_importance):\n",
    "        ax.text(v + 0.01, i, f\"{v:.3f}\", color='black', va='center')\n",
    "    \n",
    "    # Add units to x-axis if available\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title(f'Feature Importance - {best_model_name}', fontweight='bold', pad=15)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    # For linear regression\n",
    "    if hasattr(best_model, 'coef_'):\n",
    "        coefficients = pd.Series(\n",
    "            best_model.coef_,\n",
    "            index=X.columns\n",
    "        ).sort_values(ascending=True)  # Sort ascending for horizontal bar plot\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Create a horizontal bar plot\n",
    "        ax = sns.barplot(x=coefficients.values, y=coefficients.index, \n",
    "                        palette='coolwarm', orient='h')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, v in enumerate(coefficients):\n",
    "            ax.text(v, i, f\"{v:,.2f}\", color='black', va='center')\n",
    "        \n",
    "        # Add a vertical line at zero\n",
    "        plt.axvline(0, color='black', linestyle='--', linewidth=0.7)\n",
    "        \n",
    "        # Add units to x-axis\n",
    "        plt.xlabel('Coefficient Value (impact on house price in $)')\n",
    "        plt.ylabel('Features')\n",
    "        plt.title(f'Feature Coefficients - {best_model_name}', fontweight='bold', pad=15)\n",
    "        \n",
    "        # Add grid for better readability\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609dc261",
   "metadata": {},
   "source": [
    "8. Make Predictions\n",
    "-------------------\n",
    "Let's use our best model to make some example predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97f748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few random samples from the test set\n",
    "# Ensure we don't go out of bounds\n",
    "n_samples = min(5, len(X_test))\n",
    "sample_indices = np.random.choice(range(len(X_test)), size=n_samples, replace=False)\n",
    "samples = X_test.iloc[sample_indices].copy()\n",
    "samples_scaled = X_test_scaled[sample_indices]\n",
    "\n",
    "# Make predictions\n",
    "predictions = best_model.predict(samples_scaled)\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results = pd.DataFrame({\n",
    "    'Actual': y_test.iloc[sample_indices].values,\n",
    "    'Predicted': predictions,\n",
    "    'Difference': y_test.iloc[sample_indices].values - predictions\n",
    "})\n",
    "\n",
    "# Format the DataFrame for better display\n",
    "results_formatted = results.copy()\n",
    "for col in ['Actual', 'Predicted', 'Difference']:\n",
    "    results_formatted[col] = results_formatted[col].apply(lambda x: f\"${x:,.2f}\")\n",
    "\n",
    "print(\"\\nExample Predictions:\")\n",
    "print(pd.concat([samples.reset_index(drop=True), results_formatted], axis=1).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cdf405",
   "metadata": {},
   "source": [
    "9. Save the Model\n",
    "----------------\n",
    "Let's save the best model and the scaler for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2f2392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for models if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "model_path = '../models/best_model.joblib'\n",
    "joblib.dump(best_model, model_path)\n",
    "\n",
    "# Save the scaler\n",
    "scaler_path = '../models/scaler.joblib'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "print(f\"\\nBest model saved to {model_path}\")\n",
    "print(f\"Scaler saved to {scaler_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01917c87",
   "metadata": {},
   "source": [
    "10. Conclusion\n",
    "--------------\n",
    "In this demo, we've built a machine learning pipeline to predict housing prices in California.\n",
    "We've explored the data, trained multiple models, and evaluated their performance.\n",
    "The best performing model can be used to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c8b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDemo completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a594856",
   "metadata": {},
   "source": [
    "To convert this script to a Jupyter notebook, run:\n",
    "```\n",
    "jupyter nbconvert --to notebook --execute housing_price_prediction.py\n",
    "```\n",
    "\n",
    "This will create a new file called `housing_price_prediction.ipynb` that you can open and run in Jupyter."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
